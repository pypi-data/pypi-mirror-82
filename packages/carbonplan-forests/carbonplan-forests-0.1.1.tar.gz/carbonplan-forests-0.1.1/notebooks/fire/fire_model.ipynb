{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img width=\"50\" src=\"https://carbonplan-assets.s3.amazonaws.com/monogram/dark-small.png\" style=\"margin-left:0px;margin-top:20px\"/>\n",
    "\n",
    "# MTBS Fire Modeling Demo\n",
    "\n",
    "_by Jeremy Freeman (CarbonPlan), September 19, 2020_\n",
    "\n",
    "This notebook loads munged, downsampled MTBS fire data and TerraClimate climate\n",
    "data and fits a simple logistical regression model.\n",
    "\n",
    "Before getting started you'll need to download this file:\n",
    "https://storage.googleapis.com/carbonplan-data/processed/mtbs/conus/16000m/training.pkl.\n",
    "\n",
    "A file with the same contents is available in parquet:\n",
    "https://storage.googleapis.com/carbonplan-data/processed/mtbs/conus/16000m/training.parquet.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from showit import image\n",
    "import warnings\n",
    "import pickle\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.linear_model import LinearRegression, LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score\n",
    "warnings.simplefilter(action='ignore', category=RuntimeWarning)\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We've already munged the data into two dictionaries: X (with our regressors) and\n",
    "y (with our fire data). We're using the downsampled data for speed and\n",
    "simplicity. At higher resolutions, similar approaches should work, and\n",
    "additional techniques could possibly be included.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from forests import load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coarsen = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y, time = load.mtbs(store=\"local\", return_type=\"numpy\", coarsen=coarsen)\n",
    "\n",
    "X = load.terraclim(\n",
    "    store=\"local\",\n",
    "    tlim=(1984, 2018),\n",
    "    mean=False,\n",
    "    return_type=\"numpy\",\n",
    "    coarsen=coarsen,\n",
    "    vars=[\"ppt\", \"tmax\", \"tmax\", \"tmax\", \"tmin\", \"tmin\", \"tmin\"],\n",
    "    aggs=[\"sum\", \"mean\", \"min\", \"max\", \"mean\", \"min\", \"max\"],\n",
    ")\n",
    "\n",
    "mask = load.nlcd(\n",
    "    store=\"local\",\n",
    "    classes=[41, 42, 43, 51, 52, 90],\n",
    "    year=2001,\n",
    "    return_type=\"numpy\",\n",
    "    coarsen=coarsen,\n",
    ")\n",
    "X[\"forested\"] = np.tile(mask, [len(time), 1, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "allmask = load.nlcd(\n",
    "    store=\"local\",\n",
    "    classes=\"all\",\n",
    "    year=2001,\n",
    "    return_type=\"numpy\",\n",
    "    coarsen=coarsen,\n",
    ")\n",
    "for key in X.keys():\n",
    "    X[key][np.tile(allmask == 0, [len(time), 1, 1])] = np.NaN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There's one `y` variable `burned_area` and several `X` variables\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For some simple summary statistics, we can look at the spatial distribution of\n",
    "burned area averaged across years\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image((y[\"burned_area\"]).sum(axis=0), clim=(0, 1), cmap=\"hot\", size=14)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here's the max temperature (max daily maxed across months averaged across years)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image(X[\"tmax_max\"].mean(axis=0), clim=(0, 50), cmap=\"magma\", size=12)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And here's the fraction of each grid cell that is forested from NLCD\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image(X[\"forested\"].mean(axis=0), clim=(0, 1), cmap=\"bone\", size=12)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also look at dynamics over time avearged across space. We'll make a quick\n",
    "plotting function.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "zscore = lambda x: (x - x.mean()) / x.std()\n",
    "\n",
    "\n",
    "def plot_time_vars(var1, var2):\n",
    "    y1 = zscore(np.nanmean(X[var1].reshape(len(time), -1), axis=1))\n",
    "    y2 = zscore(np.nanmean(y[var2].reshape(len(time), -1), axis=1))\n",
    "    plt.plot(time, y1)\n",
    "    plt.plot(time, y2)\n",
    "    plt.legend([var1, var2])\n",
    "    plt.xlabel(\"time\")\n",
    "    plt.ylabel(f\"{var1} (zscored)\")\n",
    "    plt.figure()\n",
    "    plt.plot(y1, y2, \".\")\n",
    "    plt.xlabel(var1)\n",
    "    plt.ylabel(var2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we'll look at the positive relationship between maximum temperature and\n",
    "burned area\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_time_vars(\"tmax_max\", \"burned_area\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And now we'll look at the negative relationship with maximum precipitation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_time_vars(\"ppt_sum\", \"burned_area\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fit a model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we build a super simple space-time separable logistic regression model to\n",
    "predict fraction burned as a function of climatic variables.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we define our dimensions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = len(time)\n",
    "n, m = y[\"burned_area\"].shape[1:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are going to construct a `(t)(n)(m) x (2v - 1)` design matrix where each of\n",
    "our climatic variables appears twice, once where we have averaged across space\n",
    "and replicated across time, and again where we have averaged across time and\n",
    "replicated across space.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When building the temporal component we skip `forested` because that variable is\n",
    "constant over time.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_s = np.asarray(\n",
    "    [\n",
    "        np.tile(np.nanmean(X[var], axis=0).flatten(), [1, t]).squeeze()\n",
    "        for var in X.keys()\n",
    "    ]\n",
    ")\n",
    "\n",
    "keys = list(X.keys())\n",
    "keys.remove(\"forested\")\n",
    "X_t = np.asarray(\n",
    "    [\n",
    "        np.tile(np.nanmean(X[var].reshape(t, n * m), axis=1), [n * m, 1])\n",
    "        .T.flatten()\n",
    "        .squeeze()\n",
    "        for var in keys\n",
    "    ]\n",
    ").squeeze()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We combine them together to get our full design matrix\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_st = np.vstack([X_s, X_t]).T\n",
    "print(f\"shape: {X_st.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We turn fraction burned into a binary variable using a threshold, and flatten it\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_b = y[\"burned_area\"].flatten() > 0.05"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And we identify all rows that are nans in any of our variables\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inds = (~np.isnan(X_st.sum(axis=1))) & (~np.isnan(y_b))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally we create a function for zscoring our variables\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def zscore_2d(x, mean=None, std=None):\n",
    "    recomputing = False\n",
    "    if mean is None or std is None:\n",
    "        recomputing = True\n",
    "    if mean is None:\n",
    "        mean = x.mean(axis=0)\n",
    "    if std is None:\n",
    "        std = x.std(axis=0)\n",
    "    if recomputing:\n",
    "        return mean, std, (x - mean) / std\n",
    "    else:\n",
    "        return (x - mean) / std"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's define and fit the model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_mean, train_std, zscored = zscore_2d(X_st[inds])\n",
    "model = LogisticRegression(fit_intercept=True, max_iter=500, solver=\"lbfgs\")\n",
    "model.fit(zscored, y_b[inds])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's inspect the predictions by computing estimated fraction burned\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_hat = model.predict_proba(zscored)[:, 1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can calculate a quick and dirty ROC score `WARNING NOT CROSS VALIDATED!`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "roc_auc_score(y_b[inds], y_hat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To inspect it visually, we need to reconstitute the full tensor by inverting our\n",
    "NaN removal step\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_hat_full = np.zeros((t, n, m)).flatten()\n",
    "y_hat_full[inds] = y_hat\n",
    "y_hat_full = y_hat_full.reshape(t, n, m)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we'll look at the model's prediction across time\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(time, np.nanmean(y_b.reshape(t, n * m), axis=1))\n",
    "plt.plot(time, np.nanmean(y_hat_full.reshape(t, n * m), axis=1))\n",
    "plt.legend([\"data\", \"model\"])\n",
    "plt.xlabel(\"time\")\n",
    "plt.ylabel(\"fraction burned\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And then we can do the same thing across space\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image(np.nanmean(y_hat_full, axis=0), size=12, clim=(0, 0.1), cmap=\"hot\")\n",
    "image(\n",
    "    np.nanmean(y_b.reshape(t, n, m), axis=0), clim=(0, 0.1), cmap=\"hot\", size=12\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, as a sanity check, we can show that if we use only the spatial or\n",
    "temporal component the fit gets worse, and the latter is much worse given that\n",
    "the total dataset varies much more substantially over space than time\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(zscore_2d(X_s.T[inds]), y_b[inds])\n",
    "y_hat_s = model.predict_proba(zscore_2d(X_s.T[inds]))[:, 1]\n",
    "score_s = roc_auc_score(y_b[inds], y_hat_s)\n",
    "print(f\"spatial only: {score_s}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(zscore_2d(X_t.T[inds]), y_b[inds])\n",
    "y_hat_t = model.predict_proba(zscore_2d(X_t.T[inds]))[:, 1]\n",
    "score_t = roc_auc_score(y_b[inds], y_hat_t)\n",
    "print(f\"temporal only: {score_t}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Projections\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n, m = y[\"burned_area\"].shape[1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scenarios = [\"ssp245\", \"ssp370\", \"ssp585\"]\n",
    "for scenario in scenarios:\n",
    "    Xhat = load.cmip(\n",
    "        model=\"BCC-CSM2-MR\",\n",
    "        scenario=scenario,\n",
    "        coarsen=coarsen,\n",
    "        store=\"local\",\n",
    "        tlim=(2015, 2100),\n",
    "        mean=False,\n",
    "        return_type=\"numpy\",\n",
    "        vars=[\"ppt\", \"tmax\", \"tmax\", \"tmax\", \"tmin\", \"tmin\", \"tmin\"],\n",
    "        aggs=[\"sum\", \"mean\", \"min\", \"max\", \"mean\", \"min\", \"max\"],\n",
    "    )\n",
    "\n",
    "    t = Xhat[\"tmax_max\"].shape[0]\n",
    "    mask = load.nlcd(\n",
    "        store=\"local\",\n",
    "        classes=[41, 42, 43, 51, 52, 90],\n",
    "        year=2001,\n",
    "        coarsen=coarsen,\n",
    "        return_type=\"numpy\",\n",
    "    )\n",
    "    Xhat[\"forested\"] = np.tile(mask, [t, 1, 1])\n",
    "\n",
    "    allmask = load.nlcd(\n",
    "        store=\"local\",\n",
    "        classes=\"all\",\n",
    "        year=2001,\n",
    "        return_type=\"numpy\",\n",
    "        coarsen=coarsen,\n",
    "    )\n",
    "    for key in Xhat.keys():\n",
    "        Xhat[key][np.tile(allmask == 0, [t, 1, 1])] = np.NaN\n",
    "\n",
    "    Xhat_s = np.asarray(\n",
    "        [\n",
    "            np.tile(np.nanmean(Xhat[var], axis=0).flatten(), [1, t]).squeeze()\n",
    "            for var in Xhat.keys()\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    keys = list(Xhat.keys())\n",
    "    keys.remove(\"forested\")\n",
    "    Xhat_t = np.asarray(\n",
    "        [\n",
    "            np.tile(np.nanmean(Xhat[var].reshape(t, n * m), axis=1), [n * m, 1])\n",
    "            .T.flatten()\n",
    "            .squeeze()\n",
    "            for var in keys\n",
    "        ]\n",
    "    ).squeeze()\n",
    "\n",
    "    Xhat_st = np.vstack([Xhat_s, Xhat_t]).T\n",
    "    inds = ~np.isnan(Xhat_st.sum(axis=1))\n",
    "    y_proj = model.predict_proba(\n",
    "        zscore_2d(Xhat_st[inds], mean=train_mean, std=train_std)\n",
    "    )[:, 1]\n",
    "    y_proj_full = np.zeros((t, n, m)).flatten()\n",
    "    y_proj_full[inds] = y_proj\n",
    "    y_proj_full = y_proj_full.reshape(t, n, m).squeeze()\n",
    "\n",
    "    mask2 = load.nlcd(\n",
    "        store=\"local\",\n",
    "        classes=[41, 42, 43, 90],\n",
    "        coarsen=coarsen,\n",
    "        year=2001,\n",
    "        return_type=\"numpy\",\n",
    "    )\n",
    "    mask2 = mask2 > 0.5\n",
    "\n",
    "    y_proj_full = y_proj_full * np.tile(mask2, [t, 1, 1])\n",
    "\n",
    "    plt.plot(np.nanmean(y_proj_full.reshape(86, 195 * 302), axis=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "yt = np.nanmean(y_proj_full.reshape(86, 195 * 302), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image(y_proj_full[0:10].mean(axis=0), clim=(0, 0.5), cmap=\"hot\", size=12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = y_proj_full\n",
    "image(a, clim=(0, 0.5), cmap=\"hot\", size=12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image(\n",
    "    mask2 * y_proj_full[70:80].mean(axis=0),\n",
    "    clim=(0.05, 0.5),\n",
    "    cmap=\"hot\",\n",
    "    size=12,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(np.nanmean(y_proj_full.reshape(86, 195 * 302), axis=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(\n",
    "    X[\"tmax_max\"].mean(axis=0).flatten(),\n",
    "    Xhat[\"tmax_max\"].mean(axis=0).flatten(),\n",
    ")\n",
    "plt.axis(\"equal\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Todo\n",
    "\n",
    "This was just a demo. Lots more to do! Including:\n",
    "\n",
    "- Proper performance evaluation and cross validation\n",
    "- Variable selection\n",
    "- Exploration of other variables (we have many more)\n",
    "- Use of models that treat time / space more explicitly (e.g. via spatial\n",
    "  kernels and convolution)\n",
    "- Use of basis functions (e.g. GAMs)\n",
    "- Running at full resolution\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
